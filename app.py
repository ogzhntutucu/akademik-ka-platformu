import streamlit as st
import arxiv
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
import networkx as nx
import re
import requests
from bs4 import BeautifulSoup
from collections import Counter
from textblob import TextBlob

st.set_page_config(page_title="Akademik KeÅŸif Platformu", layout="wide", page_icon="ğŸ“")

st.markdown("""
    <style>
    .block-container {padding-top: 1rem; padding-bottom: 2rem;}
    div[data-testid="stMetricValue"] {font-size: 24px;}
    
    .tech-info {
        background-color: var(--secondary-background-color); /* Tema uyumlu arka plan */
        color: var(--text-color); /* Tema uyumlu yazÄ± rengi */
        padding: 15px;
        border-radius: 10px;
        border-left: 5px solid #4b7bec;
        font-size: 14px;
        margin-top: 20px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    </style>
    """, unsafe_allow_html=True)

st.title("ğŸ“ Akademik KeÅŸif Platformu")
st.markdown("""
**AmaÃ§:** Akademik literatÃ¼rÃ¼ API ile taramak, gÃ¶rselleÅŸtirmek, NLP ile duygu analizi yapmak ve Web Scraping ile atÄ±f verisi Ã¼retmektir.
""")

if 'arxiv_data' not in st.session_state:
    st.session_state['arxiv_data'] = pd.DataFrame()
if 'search_performed' not in st.session_state:
    st.session_state['search_performed'] = False
if 'bibtex_result' not in st.session_state:
    st.session_state['bibtex_result'] = None

with st.sidebar:
    st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/ArXiv_logo_2022.svg/320px-ArXiv_logo_2022.svg.png", width=150)
    st.header("ğŸ” Analiz Parametreleri")
    
    st.info("â„¹ï¸ Sistem, trend analizi iÃ§in en alakalÄ± sonuÃ§larÄ± (Relevance) otomatik getirir.")

    with st.form(key='search_form'):
        keyword = st.text_input("AraÅŸtÄ±rma Konusu", value="Generative AI")
        max_results = st.slider("Maksimum Makale SayÄ±sÄ±", 10, 100, 50)
        submit_search = st.form_submit_button("ğŸš€ Analizi BaÅŸlat")
    
    st.caption("Bu proje Dr. Ã–ÄŸretim Ãœyesi Halil Ä°brahim Okur rehberliÄŸinde **MÃ¼hendislikte Bilgisayar UygulamalarÄ± I** dersi kapsamÄ±nda OÄŸuzhan Tutucu tarafÄ±ndan geliÅŸtirildi.")

def extract_balanced_bibtex(text):
    """BibTeX parantez dengeleyici."""
    start_index = text.find('@')
    if start_index == -1: return None
    balance = 0
    started = False
    for i in range(start_index, len(text)):
        char = text[i]
        if char == '{':
            balance += 1
            started = True
        elif char == '}':
            balance -= 1
        if started and balance == 0:
            return text[start_index : i+1]
    return None

def scrape_bibtex(paper_id):
    """BibTeX Scraper."""
    clean_id = re.sub(r'v\d+$', '', paper_id)
    url = f"https://export.arxiv.org/bibtex/{clean_id}"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            bibtex_div = soup.find('div', id='bibtex')
            if bibtex_div: return bibtex_div.text.strip(), "BaÅŸarÄ±lÄ± (Div KaynaÄŸÄ±)"
            text_content = soup.get_text()
            extracted_bib = extract_balanced_bibtex(text_content)
            if extracted_bib: return extracted_bib, "BaÅŸarÄ±lÄ± (Metin Analizi)"
            return None, "Format bulunamadÄ±."
        else: return None, f"Hata: {response.status_code}"
    except Exception as e: return None, f"Hata: {str(e)}"

def get_arxiv_data(query, max_res):
    client = arxiv.Client()
    search = arxiv.Search(query=query, max_results=max_res, sort_by=arxiv.SortCriterion.Relevance)
    
    data = []
    for r in client.results(search):
        paper_id = r.entry_id.split('/')[-1]
        data.append({
            "Tarih": r.published.date(),
            "YÄ±l": r.published.year,
            "BaÅŸlÄ±k": r.title,
            "Ã–zet": r.summary.replace("\n", " "),
            "Yazarlar": [a.name for a in r.authors],
            "Ana Kategori": r.primary_category,
            "Link": r.entry_id,
            "ID": paper_id
        })
    return pd.DataFrame(data)

def plot_sentiment_analysis(df):
    """NLP Duygu Analizi."""
    df['Polarity'] = df['Ã–zet'].apply(lambda x: TextBlob(x).sentiment.polarity)
    
    def get_sentiment_label(score):
        if score > 0.05: return "Pozitif (Umut Verici)"
        elif score < -0.05: return "Negatif (Kritik/Sorun OdaklÄ±)"
        else: return "NÃ¶tr (Teknik/TanÄ±msal)"
    
    df['Duygu'] = df['Polarity'].apply(get_sentiment_label)
    
    sentiment_counts = df['Duygu'].value_counts().reset_index()
    sentiment_counts.columns = ['Duygu', 'Makale SayÄ±sÄ±']
    
    fig = px.pie(sentiment_counts, values='Makale SayÄ±sÄ±', names='Duygu', 
                 title="LiteratÃ¼rÃ¼n Duygu Durumu (Abstract Sentiment)",
                 color='Duygu',
                 color_discrete_map={
                     "Pozitif (Umut Verici)": "#2ecc71",
                     "NÃ¶tr (Teknik/TanÄ±msal)": "#95a5a6",
                     "Negatif (Kritik/Sorun OdaklÄ±)": "#e74c3c"
                 },
                 hole=0.4)
    st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("#### ğŸ” Ã–rnek Ä°nceleme")
    col1, col2 = st.columns(2)
    with col1:
        top_pos = df.nlargest(1, 'Polarity').iloc[0]
        st.info(f"**En Pozitif Makale:**\n{top_pos['BaÅŸlÄ±k']}")
    with col2:
        top_neg = df.nsmallest(1, 'Polarity').iloc[0]
        st.error(f"**En Kritik Makale:**\n{top_neg['BaÅŸlÄ±k']}")

def plot_trend_line(df):
    year_counts = df['YÄ±l'].value_counts().reset_index()
    year_counts.columns = ['YÄ±l', 'Makale SayÄ±sÄ±']
    year_counts = year_counts.sort_values('YÄ±l')
    fig = px.area(year_counts, x='YÄ±l', y='Makale SayÄ±sÄ±', markers=True, title="YÄ±llara GÃ¶re YayÄ±n Trendi")
    st.plotly_chart(fig, use_container_width=True)

def analyze_keywords(df):
    text = " ".join(df['Ã–zet'].tolist())
    text = re.sub(r'[^a-zA-Z\s]', '', text).lower()
    custom_stopwords = set(STOPWORDS)
    custom_stopwords.update(["paper", "proposed", "method", "result", "model", "based", "approach", "using", "show", "performance"])
    
    words = text.split()
    filtered_words = [w for w in words if w not in custom_stopwords and len(w) > 2]
    word_counts = Counter(filtered_words)
    
    wordcloud = WordCloud(width=800, height=350, background_color='white', stopwords=custom_stopwords, colormap='viridis').generate(text)
    return wordcloud, word_counts

def plot_optimized_network(df):
    """Plotly ile Ä°nteraktif AÄŸ Analizi"""
    G = nx.Graph()
    for authors in df['Yazarlar']:
        if len(authors) > 1:
            for i in range(len(authors)):
                for j in range(i + 1, len(authors)):
                    if G.has_edge(authors[i], authors[j]): 
                        G[authors[i]][authors[j]]['weight'] += 1
                    else: 
                        G.add_edge(authors[i], authors[j], weight=1)
    
    if len(G.nodes) > 0:
        if len(G.nodes) > 30:
            degrees = dict(G.degree)
            top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:30]
            G = G.subgraph(top_nodes)
        
        pos = nx.spring_layout(G, k=0.5, seed=42)
        
        edge_x = []
        edge_y = []
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])

        edge_trace = go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=0.5, color='#888'),
            hoverinfo='none',
            mode='lines')

        node_x = []
        node_y = []
        node_text = [] 
        node_adjacencies = []
        
        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            node_text.append(node)
            node_adjacencies.append(len(G.adj[node]))

        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode='markers', 
            hoverinfo='text',
            text=node_text,
            marker=dict(
                showscale=True,
                colorscale='YlGnBu', 
                reversescale=True,
                color=node_adjacencies, 
                size=20, 
                colorbar=dict(
                    thickness=15,
                    title='BaÄŸlantÄ± SayÄ±sÄ±',
                    xanchor='left'
                ),
                line_width=2))

        fig = go.Figure(data=[edge_trace, node_trace],
                     layout=go.Layout(
                        title={
                            'text': '<br>Akademik Ä°ÅŸ BirliÄŸi AÄŸÄ± (Ä°nteraktif)',
                            'font': {'size': 16}
                        },
                        showlegend=False,
                        hovermode='closest',
                        margin=dict(b=20,l=5,r=5,t=40),
                        annotations=[ dict(
                            text="Ä°simleri gÃ¶rmek iÃ§in noktalarÄ±n Ã¼zerine geliniz.",
                            showarrow=False,
                            xref="paper", yref="paper",
                            x=0.005, y=-0.002 ) ],
                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                        )
        
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("Bu veri setinde yeterli yazar iÅŸ birliÄŸi bulunamadÄ±.")

if submit_search:
    with st.spinner('Veriler API ile Ã§ekiliyor...'):
        df_new = get_arxiv_data(keyword, max_results)
        st.session_state['arxiv_data'] = df_new
        st.session_state['search_performed'] = True
        st.session_state['bibtex_result'] = None

if st.session_state.get('search_performed') and not st.session_state['arxiv_data'].empty:
    df = st.session_state['arxiv_data']
    
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Toplam Makale", len(df))
    unique_authors = set([a for sublist in df['Yazarlar'] for a in sublist])
    col2.metric("FarklÄ± Yazar", len(unique_authors))
    col3.metric("En Aktif YÄ±l", int(df['YÄ±l'].mode()[0]) if not df['YÄ±l'].mode().empty else 0)
    col4.metric("Kategori SayÄ±sÄ±", df['Ana Kategori'].nunique())

    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "ğŸ“ˆ Trend Analizi", 
        "â˜ï¸ Konu Modelleme", 
        "ğŸ§  Duygu Analizi", 
        "ğŸ•¸ï¸ Yazar AÄŸÄ±", 
        "ğŸ“„ DetaylÄ± Veri Seti",
        "ğŸ•·ï¸ BibTeX Scraping"
    ])
    
    with tab1:
        st.subheader("Zaman Ä°Ã§indeki YayÄ±n EÄŸilimi")
        plot_trend_line(df)
        st.markdown("""
        <div class="tech-info">
        <b>ğŸ› ï¸ Teknik AltyapÄ± ve Metodoloji</b><br>
        <ul>
        <li><b>Nedir?</b> SeÃ§ilen araÅŸtÄ±rma konusunun yÄ±llara gÃ¶re yayÄ±nlanma sÄ±klÄ±ÄŸÄ±nÄ± gÃ¶steren bir zaman serisi analizidir.</li>
        <li><b>Neden?</b> Bir teknolojinin veya akademik konunun yÃ¼kseliÅŸte mi (Trending) yoksa dÃ¼ÅŸÃ¼ÅŸte mi olduÄŸunu tespit etmek iÃ§in kullanÄ±lÄ±r.</li>
        <li><b>NasÄ±l?</b> Ã‡ekilen veriler Pandas kÃ¼tÃ¼phanesi ile <code>groupby('YÄ±l')</code> iÅŸlemi uygulanarak gruplandÄ±rÄ±lÄ±r ve <code>Plotly Area Chart</code> ile gÃ¶rselleÅŸtirilir.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with tab2:
        st.subheader("Ã–zet Analizi ve Kelime FrekanslarÄ±")
        wc, counts = analyze_keywords(df)
        fig, ax = plt.subplots(figsize=(10, 4))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
        plt.close(fig)
        
        st.markdown("#### ğŸ”¢ En SÄ±k GeÃ§en Kelimeler (Top 50)")
        common_words_df = pd.DataFrame(counts.most_common(50), columns=['Kelime', 'Frekans'])
        st.dataframe(common_words_df, use_container_width=True, height=300)

        st.markdown("""
        <div class="tech-info">
        <b>ğŸ› ï¸ Teknik AltyapÄ± ve Metodoloji</b><br>
        <ul>
        <li><b>Nedir?</b> Makale Ã¶zetlerinden (Abstract) en sÄ±k kullanÄ±lan terimlerin Ã§Ä±karÄ±lmasÄ± iÅŸlemidir (Topic Modeling).</li>
        <li><b>Neden?</b> LiteratÃ¼rdeki alt Ã§alÄ±ÅŸma alanlarÄ±nÄ± ve popÃ¼ler terminolojiyi belirlemek iÃ§in kullanÄ±lÄ±r.</li>
        <li><b>NasÄ±l?</b> Metinler Ã¶nce Regex ile temizlenir, Stopwords (etkisiz kelimeler) Ã§Ä±karÄ±lÄ±r ve <code>Counter</code> ile frekans analizi yapÄ±lÄ±r. SonuÃ§lar <code>WordCloud</code> kÃ¼tÃ¼phanesi ile gÃ¶rselleÅŸtirilir.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

    with tab3:
        st.subheader("NLP ile Ã–zet Duygu Analizi (Sentiment Analysis)")
        st.markdown("Makale Ã¶zetleri, **DoÄŸal Dil Ä°ÅŸleme (NLP)** kullanÄ±larak analiz edilmiÅŸtir.")
        plot_sentiment_analysis(df)

        st.markdown("""
        <div class="tech-info">
        <b>ğŸ› ï¸ Teknik AltyapÄ± ve Metodoloji</b><br>
        <ul>
        <li><b>Nedir?</b> Akademik metinlerin dilinin pozitif (baÅŸarÄ±lÄ±/umut verici) mi yoksa negatif (sorun odaklÄ±/kritik) mi olduÄŸunu analiz eden bir NLP sÃ¼recidir.</li>
        <li><b>Neden?</b> LiteratÃ¼rÃ¼n genel atmosferini ve araÅŸtÄ±rmacÄ±larÄ±n konuya yaklaÅŸÄ±mÄ±nÄ± anlamak iÃ§in kullanÄ±lÄ±r.</li>
        <li><b>NasÄ±l?</b> Python <code>TextBlob</code> kÃ¼tÃ¼phanesi kullanÄ±larak her Ã¶zet iÃ§in bir Polarity skoru (-1 ile +1 arasÄ±) hesaplanÄ±r ve kategorize edilir.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

    with tab4:
        st.subheader("Akademik Ä°ÅŸ BirliÄŸi AÄŸÄ±")
        plot_optimized_network(df)

        st.markdown("""
        <div class="tech-info">
        <b>ğŸ› ï¸ Teknik AltyapÄ± ve Metodoloji</b><br>
        <ul>
        <li><b>Nedir?</b> Yazarlar arasÄ±ndaki ortak Ã§alÄ±ÅŸma (Co-authorship) iliÅŸkilerini gÃ¶steren bir grafik teorisi uygulamasÄ±dÄ±r.</li>
        <li><b>Neden?</b> AlanÄ±n en Ã¼retken gruplarÄ±nÄ±, merkezi yazarlarÄ± (Hubs) ve iÅŸ birliÄŸi kÃ¼melerini keÅŸfetmek iÃ§in kullanÄ±lÄ±r.</li>
        <li><b>NasÄ±l?</b> <code>NetworkX</code> kÃ¼tÃ¼phanesi ile dÃ¼ÄŸÃ¼mler (Yazarlar) ve kenarlar (Ortak Makaleler) oluÅŸturulur. Fruchterman-Reingold algoritmasÄ± ile yerleÅŸim yapÄ±lÄ±r ve <code>Plotly</code> ile interaktif hale getirilir.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
        
    with tab5:
        st.subheader("Ham Veri Tablosu")
        display_df = df[['Tarih', 'Ana Kategori', 'BaÅŸlÄ±k', 'Yazarlar', 'Ã–zet', 'Link']]
        st.dataframe(display_df, use_container_width=True)
        
        csv = display_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ğŸ“¥ Veri Setini CSV Olarak Ä°ndir",
            data=csv,
            file_name=f'{keyword}_arxiv_data.csv',
            mime='text/csv',
        )

        st.markdown("""
        <div class="tech-info">
        <b>ğŸ› ï¸ Teknik AltyapÄ± ve Metodoloji</b><br>
        <ul>
        <li><b>Nedir?</b> Analiz edilen tÃ¼m verinin yapÄ±landÄ±rÄ±lmÄ±ÅŸ (Structured) ham halidir.</li>
        <li><b>Neden?</b> ÅeffaflÄ±k saÄŸlamak ve verilerin baÅŸka araÃ§larda (Excel, SPSS) kullanÄ±labilmesine olanak tanÄ±mak iÃ§in.</li>
        <li><b>NasÄ±l?</b> Veriler <code>Pandas DataFrame</code> objesinde tutulur ve <code>to_csv</code> fonksiyonu ile UTF-8 formatÄ±nda dÄ±ÅŸa aktarÄ±lÄ±r.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

    with tab6:
        st.subheader("ğŸ“š Otomatik BibTeX OluÅŸturucu")
        st.success("Web Scraping modÃ¼lÃ¼, ArXiv'in export sunucularÄ±na baÄŸlanarak atÄ±f verisini doÄŸrular.")
        
        with st.form(key='scrape_form'):
            selected_paper = st.selectbox("KaynakÃ§asÄ± oluÅŸturulacak makaleyi seÃ§iniz:", df['BaÅŸlÄ±k'])
            scrape_btn = st.form_submit_button("BibTeX Kodunu KazÄ± (Scrape)")

        if scrape_btn:
            paper_id = df[df['BaÅŸlÄ±k'] == selected_paper]['ID'].values[0]
            with st.spinner("ArXiv sunucularÄ±na baÄŸlanÄ±lÄ±yor..."):
                bibtex_code, status = scrape_bibtex(paper_id)
                st.session_state['bibtex_result'] = (bibtex_code, status, selected_paper)
        
        if st.session_state['bibtex_result']:
            code, stat, title = st.session_state['bibtex_result']
            st.markdown(f"**SeÃ§ilen Makale:** {title}")
            
            if code:
                st.success(f"âœ… Scraping BaÅŸarÄ±lÄ±! ({stat})")
                st.code(code, language='latex')
            else:
                st.error(f"âŒ {stat}")
        
        st.markdown("""
        <div class="tech-info">
        <b>ğŸ› ï¸ Teknik AltyapÄ± ve Metodoloji</b><br>
        <ul>
        <li><b>Nedir?</b> ArXiv API'sinin saÄŸlamadÄ±ÄŸÄ± BibTeX (LaTeX AtÄ±f FormatÄ±) verisinin web sitesinden canlÄ± olarak Ã§ekilmesidir.</li>
        <li><b>Neden?</b> AraÅŸtÄ±rmacÄ±larÄ±n makaleyi kaynakÃ§alarÄ±na ekleyebilmesi iÃ§in gereklidir.</li>
        <li><b>NasÄ±l?</b> Python <code>Requests</code> ve <code>BeautifulSoup</code> kÃ¼tÃ¼phaneleri kullanÄ±larak HTML ayrÄ±ÅŸtÄ±rma (Web Scraping) yapÄ±lÄ±r. Regex ve metin analizi ile BibTeX bloÄŸu tespit edilip temizlenir.</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)